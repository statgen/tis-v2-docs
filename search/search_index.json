{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#topmed-imputation-server","title":"TOPMed imputation Server","text":"<p>The TOPMed Imputation Server provides a free genotype imputation service (chromosomes 1-22 and X) using Minimac4. You can upload phased or unphased GWAS genotypes and receive phased and imputed genomes in return. This server offers imputation from the TOPMed reference panel. For all uploaded datasets, comprehensive QC is performed.</p> <p>Please cite the following papers if you use the TOPMed Imputation Server:</p> <ul> <li>TOPMed Study: <p>Taliun, D. et al. (2019) Sequencing of 53,831 diverse genomes from the NHLBI TOPMed Program. Biorxiv, doi:10.1101/563866</p> </li> <li>Imputation Server: <p>Das S, Forer L, Sch\u00f6nherr S, Sidore C, Locke AE, Kwong A, Vrieze S, Chew EY, Levy S, McGue M, Schlessinger D, Stambolian D, Loh PR, Iacono WG, Swaroop A, Scott LJ, Cucca F, Kronenberg F, Boehnke M, Abecasis GR, Fuchsberger C. Next-generation genotype imputation service and methods. Nature Genetics 48, 1284\u20131287 (2016).</p> </li> <li>Minimac Imputation: <p>Fuchsberger, C., Abecasis, G. R., &amp; Hinds, D. A. (2014). minimac2: faster genotype imputation. Bioinformatics, 31(5), 782\u2013784.</p> </li> </ul>"},{"location":"about/","title":"About","text":""},{"location":"about/#genotype-imputation","title":"Genotype Imputation","text":"<p>Genotype imputation is a key component of genetic association studies, where it increases power, facilitates meta-analysis, and aids interpretation of signals. This imputation server allows registered users to perform genotype imputation for their own samples. Typically, this allows tens of millions of genetic variants to be genotyped with very high-fidelity in samples that have been genotyped with an affordable genotyping array. Imputation works by comparing short stretches of an individual genome to stretches of previously characterized genomes. Since all humans are related, this process can typically and accurately reconstruct much of the missing data in each genome.</p>"},{"location":"about/#the-topmed-imputation-panel-and-server","title":"The TOPMed Imputation Panel and Server","text":"<p>The TOPMed Imputation Reference panel is a diverse reference panel including information from 133,597 deeply sequenced human genomes. The panel is available to the community through a collaboration between TOPMed Study Investigators, the National Heart Lung and Blood Institute and the University of Michigan Imputation Server team.</p>"},{"location":"about/#the-topmed-study","title":"The TOPMed Study","text":"<p>The Trans-Omics for Precision Medicine (TOPMed) program seeks to elucidate the genetic architecture and disease biology of heart, lung, blood, and sleep disorders, with the ultimate goal of improving diagnosis, treatment, and prevention. The initial phases of the program included whole genome sequencing of individuals with rich phenotypic data and diverse backgrounds. This reference panel is built on a subset of 133,597 TOPMed samples for whom high quality whole genome sequence data was available and for where contributing studies approved this use of the data, taking into account both the limited risks associated with including a sample in an imputation reference panel that is only available through this imputation server and also other study and sample specific use restrictions. Version r3 of the panel, which is the current version available to the scientific community at large, includes 133,597 reference samples and 445,600,184 genetic variants distributed across the 22 autosomes and the X chromosome.</p> <p>For a more detailed description of the study, see the TOPMed pre-print currently on BioXriv Taliun, D. et al. (2019) Sequencing of 53,831 diverse genomes from the NHLBI TOPMed Program. Biorxiv, doi:10.1101/563866.</p>"},{"location":"about/#the-michigan-imputation-server","title":"The Michigan Imputation Server","text":"<p>The Michigan Imputation Server is a joint of effort of the statistical genetics teams at the University of Michigan in the U.S.A. and at the University of Innsbruck in Austria. The Michigan Imputation Server technology provides a web-based service for imputation that facilitates access to imputation reference panels and greatly improves user experience and productivity. The server maintains a private list of reference genomes, which are not directly accessible by panel users. Users can request imputation of their own samples and securely download the results. For information on imputation server technology, please see:</p> <ul> <li>Imputation Server \u2014 Das, S., Forer, L., Sch\u00f6nherr, S., Sidore, C., Locke, A. E., Kwong, A., Vrieze, S. I., Chew, E. Y., Levy, S., McGue, M., Schlessinger, D., Stambolian, D., Loh, P.-R., Iacono, W. G., Swaroop, A., Scott, L. J., Cucca, F., Kronenberg, F., Boehnke, M., \u2026 Fuchsberger, C. (2016). Next-generation genotype imputation service and methods. Nature Genetics, 48(10), 1284\u20131287.</li> <li>Eagle Haplotype Phasing \u2014 Loh, P.-R., Danecek, P., Palamara, P. F., Fuchsberger, C., A Reshef, Y., K Finucane, H., Schoenherr, S., Forer, L., McCarthy, S., Abecasis, G. R., Durbin, R., &amp; L Price, A. (2016). Reference-based phasing using the Haplotype Reference Consortium panel. Nature Genetics, 48(11), 1443\u20131448.</li> <li>Minimac Imputation \u2014 Fuchsberger, C., Abecasis, G. R., &amp; Hinds, D. A. (2014). minimac2: faster genotype imputation. Bioinformatics, 31(5), 782\u2013784.</li> </ul>"},{"location":"about/#the-national-heart-lung-and-blood-institute","title":"The National Heart Lung and Blood Institute","text":"<p>The National Heart Lung and Blood Institute is part of the National Institutes of Health and has provided key support to the development of the studies that collectively constitute the TOPMed Program. The Institute has also sponsored the sequencing and analysis of whole genome sequence and other data within the TOPMed program. NHLBI is sponsoring this imputation server and providing underlying compute resources through its BioDataCatalyst program.</p>"},{"location":"about/#citation","title":"Citation","text":"<p>If you use the Imputation Server in your work, please cite:</p> <ul> <li>TOPMed Study \u2014 Taliun, D. et al. (2019) Sequencing of 53,831 diverse genomes from the NHLBI             TOPMed Program. Biorxiv, doi:10.1101/563866</li> <li>Imputation Server \u2014 Das, S., Forer, L., Sch\u00f6nherr, S., Sidore, C., Locke, A. E., Kwong, A., Vrieze, S. I., Chew, E. Y., Levy, S., McGue, M., Schlessinger, D., Stambolian, D., Loh, P.-R., Iacono, W. G., Swaroop, A., Scott, L. J., Cucca, F., Kronenberg, F., Boehnke, M., \u2026 Fuchsberger, C. (2016). Next-generation genotype imputation service and methods. Nature Genetics, 48(10), 1284\u20131287.</li> <li>Minimac Imputation \u2014 Fuchsberger, C., Abecasis, G. R., &amp; Hinds, D. A. (2014). minimac2: faster genotype imputation. Bioinformatics, 31(5), 782\u2013784.</li> </ul>"},{"location":"about/#acknowledgements","title":"Acknowledgements","text":"<p>Trans-Omics in Precision Medicine (TOPMed) program imputation panel (version TOPMed-r3) supported by the National Heart, Lung and Blood Institute (NHLBI); see topmed.nhlbi.nih.gov. TOPMed study investigators contributed data to the reference panel, which can be accessed through the TOPMed Imputation Server. The panel was constructed and implemented by the TOPMed Informatics Research Center at the University of Michigan (3R01HL-117626-02S1; contract HHSN268201800002I). The TOPMed Data Coordinating Center (3R01HL-120393-02S1; contract HHSN268201800001I) provided additional data management, sample identity checks, and overall program coordination and support. We gratefully acknowledge the studies and participants who provided biological samples and data for TOPMed.</p>"},{"location":"contact/","title":"Contact","text":""},{"location":"contact/#general-inquiries","title":"General Inquiries","text":"<ul> <li>Imputation Server Team</li> </ul>"},{"location":"contact/#topmed-imputation-server","title":"TOPMed Imputation Server","text":"<ul> <li>Please consult the FAQ first for answers to common questions</li> <li>Helpdesk (use this address for TOPMed Imputation Server inquiries)</li> <li>Albert Smith</li> <li>Snehal Patil</li> <li>Marc Fraile</li> </ul>"},{"location":"contact/#michigan-imputation-server","title":"Michigan Imputation Server","text":"<ul> <li>Christian Fuchsberger (use this address for Michigan Imputation Server inquiries)</li> <li>Lukas Forer</li> <li>Sebastian Schoenherr</li> <li>Gon\u00e7alo Abecasis</li> </ul>"},{"location":"contact/#imputation-engine-minimac4","title":"Imputation engine: Minimac4","text":"<ul> <li>Sayantan Das</li> <li>Ketian Yu</li> <li>Gon\u00e7alo Abecasis</li> </ul>"},{"location":"contact/#cloud-framework-cloudgene","title":"Cloud framework: Cloudgene","text":"<ul> <li>Sebastian Schoenherr</li> <li>Lukas Forer</li> </ul>"},{"location":"contact/#pre-phasing-eagle","title":"Pre-phasing: Eagle","text":"<ul> <li>Po-Ru Loh</li> </ul>"},{"location":"faq/","title":"Frequently Asked Questions (FAQ)","text":""},{"location":"faq/#what-chromosomes-are-supported","title":"What chromosomes are supported?","text":"<p>The TOPMed reference panel can be used to impute autosomes (1-22) and the X chromosome. See here for more details.</p> <p>Please note that we do NOT support Y or MT chromosome processing.</p>"},{"location":"faq/#how-many-samples-can-i-submit-at-once","title":"How many samples can I submit at once?","text":"<p>The Imputation Server only accepts jobs containing between 20 and 25,000 samples. If your job is too small (&lt; 20 samples) or too large (&gt; 25,000 samples), it will be rejected.</p> <p>This limit exists to preserve quality of service for a wide audience. A workaround is to break large jobs into multiple chunks of 25,000 samples each. After completion, the results can be re-merged using hds-util to combine the chunks and calculate the corrected R2.</p> <p>If you have a use case that routinely requires smaller or larger jobs, please contact us with details.</p>"},{"location":"faq/#i-did-not-receive-a-password-for-my-job-can-you-re-send-it","title":"I did not receive a password for my job, can you re-send it?","text":"<p>We can't.</p> <p>The TOPMed Imputation Server creates a random password for each imputation job. For security reasons, this password is not stored on server-side at any time. If you didn't receive a password, please check the spam folder in your email. Please note that we are not able to re-send you the password. If you lose it, you will need to re-run your imputation job.</p>"},{"location":"faq/#i-would-like-to-impute-wgs-data-but-the-server-says-i-have-too-many-variants-what-do-i-do","title":"I would like to impute WGS data, but the server says I have too many variants. What do I do?","text":"<p>The Imputation Server is meant to fill in the gaps between sites on a genotyping array (not WGS). Due to server resource requirements, imputing samples with many variants (more than 20,000 in any given 10Mb chunk) is not supported.</p> <p>If you have WGS at 50x, then you would get little to no benefit from imputing on the server.</p>"},{"location":"faq/#how-many-jobs-can-i-submit-at-once","title":"How many jobs can I submit at once?","text":"<p>You can have up to three (3) jobs running at the same time. Please note that attempting to bypass this limit might result in a ban.</p> <p>The TOPMed imputation server is a free resource, and usage limits allow us to provide service to a wide audience. Please do not attempt to bypass these limits by creating multiple accounts. We monitor usage, and reserve the right to terminate jobs or accounts that are in violation of our policies.</p>"},{"location":"faq/#the-unzip-command-is-not-working-what-do-i-do","title":"The unzip command is not working. What do I do?","text":"<p>Please check the following points:</p> <ol> <li>When selecting AES256 encryption, please use 7-zip to unzip your files. For the default encryption option, all common <code>.zip</code> decompression programs should work.</li> <li>If your password includes special characters (e.g. <code>\\</code>), please put single or double quotes around the password when extracting it from the command line (e.g. <code>7z x -p\"PASSWORD\" chr_22.zip</code>).</li> </ol>"},{"location":"faq/#how-long-are-my-results-available","title":"How long are my results available?","text":"<p>Your data is available for 7 days. If you need to extend the due date, please let us know ahead of time. Once the data is deleted we cannot help.</p>"},{"location":"faq/#how-many-times-can-i-download-my-files","title":"How many times can I download my files?","text":"<p>There is a limit of 50 downloads per file. Please let us know if you need an extension.</p>"},{"location":"faq/#how-can-i-improve-the-download-speed","title":"How can I improve the download speed?","text":"<p>aria2 tries to utilize your maximum download bandwidth. Remember to raise the k parameter significantly (-k, --min-split-size=SIZE). You will otherwise hit the Imputation Server download limit for each file (thanks to Anthony Marcketta for pointing this out).</p>"},{"location":"faq/#can-i-download-all-results-at-once","title":"Can I download all results at once?","text":"<p>Yes! If you click the download icon in the Results tab, you will encounter <code>wget</code> and <code>curl</code> commands to easily download all the files from the same step at once. You can also use the <code>imputationbot</code> or your own scripts for parallel downloads.</p>"},{"location":"faq/#is-this-service-secure","title":"Is this service secure?","text":"<p>Due to small team size, it is difficult for us to complete detailed security questionnaires for every company or entity. However, as of May 2023, we have completed a rigorous external security review and received federal Authorization to Operate (ATO) from NHLBI/NIH. Please see our security documentation for some common information, or contact us for specific questions.</p>"},{"location":"faq/#why-do-my-vcf-43-chromosome-x-phasing-jobs-keep-failing","title":"Why do my VCF 4.3 chromosome X phasing jobs keep failing?","text":"<p>Your chromosome X phasing jobs are likely failing because the server doesn\u2019t currently support VCF version 4.3. The server attempts to rewrite chromosome X due to the PAR/non-PAR split using the htsjdk library, which isn\u2019t fully compatible with VCF 4.3, especially for chromosome X data. To resolve this issue, try changing the VCF header to version 4.2. The server supports this version and should allow your phasing jobs to complete successfully.</p>"},{"location":"api/api-old/","title":"API Reference","text":"<p>The REST APIs provide programmatic ways to submit new jobs and to download data from the TOPMed Imputation Server. It identifies users using authentication tokens, responses are provided in JSON format.</p>"},{"location":"api/api-old/#authentication","title":"Authentication","text":"<p>The TOPMed Imputation Server uses a token-based authentication. The token is required for all future interaction with the server. The token can be created and downloaded from your user profile (username -&gt; Profile):</p> <p></p> <p>For security reasons, Api Tokens are valid for 30 days. You can check the status in the web interface.</p>"},{"location":"api/api-old/#job-submission-for-whole-genome-imputation","title":"Job Submission for Whole Genome Imputation","text":"<p>The API allows to submit imputation jobs and to set several parameters.</p>"},{"location":"api/api-old/#post-jobssubmitimputationserver2","title":"POST /jobs/submit/imputationserver2","text":"<p>The following parameters can be set:</p> Parameter Values Default Value Required files /path/to/file x mode <code>qconly</code> <code>phasing</code> <code>imputation</code> <code>imputation</code> password user-defined password auto-generated and sent by mail refpanel <code>hrc-r1.1</code> <code>1000g-phase-3-v5</code> <code>gasp-v2</code> <code>genome-asia-panel</code> <code>1000g-phase-1</code> <code>cappa</code> <code>hapmap-2</code> - x phasing <code>eagle</code> <code>no_phasing</code> <code>eagle</code> population <code>eur</code> <code>afr</code> <code>asn</code> <code>amr</code> <code>sas</code> <code>eas</code> <code>AA</code> <code>mixed</code> <code>all</code> - x build <code>hg19</code> <code>hg38</code> <code>hg19</code> r2Filter <code>0</code> <code>0.001</code> <code>0.1</code> <code>0.2</code> <code>0.3</code> <code>0</code>"},{"location":"api/api-old/#examples-curl","title":"Examples: curl","text":""},{"location":"api/api-old/#submit-a-single-file","title":"Submit a single file","text":"<p>To submit a job please change <code>/path-to/file.vcf.gz</code> to a valid vcf file and update <code>TOKEN</code> with your API Token:</p> <p>Command:</p> <pre><code>TOKEN=\"YOUR-API-TOKEN\";\n\ncurl https://imputationserver.sph.umich.edu/api/v2/jobs/submit/imputationserver2 \\\n  -H \"X-Auth-Token: $TOKEN\" \\\n  -F \"files=@/path-to/file.vcf.gz\" \\\n  -F \"refpanel=1000g-phase-3-v5\" \\\n  -F \"population=eur\"\n</code></pre> <p>Response:</p> <pre><code>{\n  \"id\":\"job-20160504-161420\",\n  \"message\":\"Your job was successfully added to the job queue.\",\n  \"success\":true\n}\n</code></pre>"},{"location":"api/api-old/#submit-multiple-files","title":"Submit multiple files","text":"<p>Submits multiple vcf files and impute against 1000 Genomes Phase 3 reference panel.</p> <p>Command:</p> <pre><code>TOKEN=\"YOUR-API-TOKEN\";\n\ncurl https://imputationserver.sph.umich.edu/api/v2/jobs/submit/imputationserver2 \\\n  -H \"X-Auth-Token: $TOKEN\" \\\n  -F \"files=@/path-to/file1.vcf.gz\" \\\n  -F \"files=@/path-to/file2.vcf.gz\" \\\n  -F \"refpanel=1000g-phase-3-v5\" \\\n  -F \"population=eur\"\n</code></pre> <p>Response:</p> <pre><code>{\n  \"id\":\"job-20120504-155023\",\n  \"message\":\"Your job was successfully added to the job queue.\",\n  \"success\":true\n}\n</code></pre>"},{"location":"api/api-old/#submit-file-from-a-https","title":"Submit file from a HTTP(S)","text":"<p>Submits files from https with HRC reference panel and quality control.</p> <p>Command:</p> <pre><code>TOKEN=\"YOUR-API-TOKEN\";\n\ncurl  https://imputationserver.sph.umich.edu/api/v2/jobs/submit/imputationserver2 \\\n  -H \"X-Auth-Token: $TOKEN\" \\\n  -F \"files=https://imputationserver.sph.umich.edu/static/downloads/hapmap300.chr1.recode.vcf.gz\" \\\n  -F \"files-source=http\" \\\n  -F \"refpanel=hrc-r1.1\" \\\n  -F \"population=eur\" \\\n  -F \"mode=qconly\"\n</code></pre> <p>Response:</p> <pre><code>{\n  \"id\":\"job-20120504-155023\",\n  \"message\":\"Your job was successfully added to the job queue.\",\n  \"success\":true\n}\n</code></pre>"},{"location":"api/api-old/#examples-python","title":"Examples: Python","text":""},{"location":"api/api-old/#submit-single-vcf-file","title":"Submit single vcf file","text":"<pre><code>import requests\nimport json\n\n# imputation server url\nurl = 'https://imputationserver.sph.umich.edu/api/v2'\ntoken = 'YOUR-API-TOKEN';\n\n# add token to header (see Authentication)\nheaders = {'X-Auth-Token' : token }\ndata = {\n  'refpanel': '1000g-phase-3-v5',\n  'population': 'eur'\n}\n\n# submit new job\nvcf = '/path/to/genome.vcf.gz';\nfiles = {'files' : open(vcf, 'rb')}\nr = requests.post(url + \"/jobs/submit/imputationserver2\", files=files, data=data, headers=headers)\nif r.status_code != 200:\n  print(r.json()['message'])\n  raise Exception('POST /jobs/submit/imputationserver2 {}'.format(r.status_code))\n\n# print response and job id\nprint(r.json()['message'])\nprint(r.json()['id'])\n</code></pre>"},{"location":"api/api-old/#submit-multiple-vcf-files","title":"Submit multiple vcf files","text":"<pre><code>import requests\nimport json\n\n# imputation server url\nurl = 'https://imputationserver.sph.umich.edu/api/v2'\ntoken = 'YOUR-API-TOKEN';\n\n# add token to header (see Authentication)\nheaders = {'X-Auth-Token' : token }\ndata = {\n  'refpanel': '1000g-phase-3-v5',\n  'population': 'eur'\n}\n\n# submit new job\nvcf = '/path/to/file1.vcf.gz';\nvcf1 = '/path/to/file2.vcf.gz';\nfiles = [('files', open(vcf, 'rb')), ('files', open(vcf1, 'rb'))]\nr = requests.post(url + \"/jobs/submit/imputationserver2\", files=files, data=data, headers=headers)\nif r.status_code != 200:\n  print(r.json()['message'])\n  raise Exception('POST /jobs/submit/imputationserver2 {}'.format(r.status_code))\n\n# print message\nprint(r.json()['message'])\nprint(r.json()['id'])\n</code></pre>"},{"location":"api/api-old/#list-all-jobs","title":"List all jobs","text":"<p>All running jobs can be returned as JSON objects at once.</p>"},{"location":"api/api-old/#get-jobs","title":"GET /jobs","text":""},{"location":"api/api-old/#examples-curl_1","title":"Examples: curl","text":"<p>Command:</p> <pre><code>TOKEN=\"YOUR-API-TOKEN\";\n\ncurl -H \"X-Auth-Token: $TOKEN\" https://imputationserver.sph.umich.edu/api/v2/jobs\n</code></pre> <p>Response:</p> <pre><code>[\n  {\n    \"applicationId\":\"minimac\",\n    \"executionTime\":0,\n    \"id\":\"job-20160504-155023\",\n    \"name\":\"job-20160504-155023\",\n    \"positionInQueue\":0,\n    \"running\":false,\n    \"state\":5\n  },{\n    \"applicationId\":\"minimac\",\n    \"executionTime\":0,\n    \"id\":\"job-20160420-145809\",\n    \"name\":\"job-20160420-145809\",\n    \"positionInQueue\":0,\n    \"running\":false,\n    \"state\":5\n  },{\n    \"applicationId\":\"minimac\",\n    \"executionTime\":0,\n    \"id\":\"job-20160420-145756\",\n    \"name\":\"job-20160420-145756\",\n    \"positionInQueue\":0,\n    \"running\":false,\n    \"state\":5\n  }\n]\n</code></pre>"},{"location":"api/api-old/#example-python","title":"Example: Python","text":"<pre><code>import requests\nimport json\n\n# imputation server url\nurl = 'https://imputationserver.sph.umich.edu/api/v2'\ntoken = 'YOUR-API-TOKEN'\n\n# add token to header (see authentication)\nheaders = {'X-Auth-Token' : token }\n\n# get all jobs\nr = requests.get(url + \"/jobs\", headers=headers)\nif r.status_code != 200:\n    raise Exception('GET /jobs/ {}'.format(r.status_code))\n\n# print all jobs\nfor job in r.json():\n    print('{} [{}]'.format(job['id'], job['state']))\n</code></pre>"},{"location":"api/api-old/#monitor-job-status","title":"Monitor Job Status","text":""},{"location":"api/api-old/#jobsidstatus","title":"/jobs/{id}/status","text":""},{"location":"api/api-old/#example-curl","title":"Example: curl","text":"<p>Command:</p> <pre><code>TOKEN=\"YOUR-API-TOKEN\";\n\ncurl -H \"X-Auth-Token: $TOKEN\" https://imputationserver.sph.umich.edu/api/v2/jobs/job-20160504-155023/status\n</code></pre> <p>Response:</p> <pre><code>{\n  \"application\":\"Michigan Imputation Server (Minimac4) 1.5.8\",\n  \"applicationId\":\"minimac4\",\n  \"deletedOn\":-1,\n  \"endTime\":1462369824173,\n  \"executionTime\":0,\n  \"id\":\"job-20160504-155023\",\n  \"logs\":\"\",\n  \"name\":\"job-20160504-155023\",\n  \"outputParams\":[],\n  \"positionInQueue\":0,\n  \"running\":false,\n  \"startTime\":1462369824173,\n  \"state\":5\n  ,\"steps\":[]\n}\n</code></pre>"},{"location":"api/api-old/#monitor-job-details","title":"Monitor Job Details","text":""},{"location":"api/api-old/#jobsid","title":"/jobs/{id}","text":""},{"location":"api/api-old/#example-curl_1","title":"Example: curl","text":"<pre><code>TOKEN=\"YOUR-API-TOKEN\";\n\ncurl -H \"X-Auth-Token: $TOKEN\" https://imputationserver.sph.umich.edu/api/v2/jobs/job-20160504-155023/\n</code></pre>"},{"location":"api/api-overview/","title":"API Overview","text":""},{"location":"api/api-overview/#its-a-rest-api","title":"It's a REST API","text":"<p>The TOPMed Imputation Server exposes a REST API you can use directly to do tasks such as:</p> <ul> <li>Submit a new imputation job.</li> <li>See your current and past jobs.</li> <li>Get detailed information about a specific job.</li> </ul> <p>You send HTTP requests to our API endpoints using any tool of your liking (CURL, Python with requests, the imputationbot...), and we respond with a JSON object.</p> <p>The base URL for all calls to the API is:</p> <pre><code>BASE_URL = https://imputation.biodatacatalyst.nhlbi.nih.gov/api/v2/\n</code></pre> <p>So if we say an endpoint is <code>/jobs</code>, the full URL you need to call is <code>https://imputation.biodatacatalyst.nhlbi.nih.gov/api/v2/jobs</code>.</p> <p>With the exception of file download links, all endpoints return JSON payloads.</p>"},{"location":"api/api-overview/#token-based-authentication","title":"Token-Based Authentication","text":"<p>The TOPMed Imputation Server uses bearer tokens for authentication. This means the server gives you a secret string, and you must submit that string with every query to prove your identity.</p> <p>Keep your token safe!</p> <p>Anyone who has your token can impersonate you when communicating with the server, so ensure you protect it as well as you would protect a password!</p> <p>The token can be created and downloaded from your user profile (<code>&lt;username&gt; -&gt; Profile</code>):</p> <p></p> <p>Some security measures are in place to protect your account:</p> <ul> <li>Tokens are valid for 30 days. Once your token expires, you'll need to create a new one.</li> <li>You can only view the token once, when you request it. Make sure to copy it!</li> <li>You can only have one active token at a time.</li> </ul> <p>You can check the status of your token in the web interface. You can revoke your token and get a new one at any time.</p>"},{"location":"api/api-overview/#accessing-from-the-cli-curl-and-jq","title":"Accessing from the CLI: <code>curl</code> and <code>jq</code>","text":"<p>The full API specification (with examples using <code>curl</code> and Python) can be found in the API reference page. Here we summarize some basic examples using <code>curl</code> and <code>jq</code>.</p>"},{"location":"api/api-overview/#listing-your-jobs","title":"Listing Your Jobs","text":"<p>curl is an ubiquitous command-line utility used for interacting with REST APIs (among other things). To submit an HTTP GET request, you just write:</p> <pre><code>curl &lt;url&gt;\n</code></pre> <p>You can use the <code>-H</code> flag to set a request header. If you have saved your access token in an environment variable called <code>TIS_TOKEN</code>, you can list all your jobs by calling <code>GET /jobs</code>:</p> <pre><code>curl \\\n    -H \"X-Auth-Token: ${TIS_TOKEN}\" \\\n    'https://imputation.biodatacatalyst.nhlbi.nih.gov/api/v2/jobs'\n</code></pre> <p>This command will print compact JSON to the terminal, but it might be an unreadable wall of text. You can parse it with jq:</p> <pre><code>curl \\\n    -H \"X-Auth-Token: ${TIS_TOKEN}\" \\\n    'https://imputation.biodatacatalyst.nhlbi.nih.gov/api/v2/jobs' \\\n    | jq\n</code></pre> <p>Then you'll see some pretty-printed output along these lines:</p> <pre><code>{\n    \"count\": 32,\n    \"page\": 3,\n    \"pageSize\": 15,\n    \"data\": [\n        {\n            \"id\": \"job-20250829-110739-457\",\n            \"name\": \"My Job!\",\n            \"state\": 7,\n            \"positionInQueue\": -1,\n            \"submittedOn\": 1756480059559,\n            \"startTime\": 1756480059670,\n            \"endTime\": 1756480937680,\n            \"deletedOn\": 1757087819500,\n            \"steps\": [],\n            \"outputParams\": [],\n        },\n        ...\n    ]\n}\n</code></pre> <p><code>jq</code> offers filtering options, so for example if you only want to know the status of your jobs you can write</p> <pre><code>curl \\\n    -H \"X-Auth-Token: ${TIS_TOKEN}\" \\\n    'https://imputation.biodatacatalyst.nhlbi.nih.gov/api/v2/jobs' \\\n    | jq '.data[] | { id, state }'\n</code></pre> <p>It will print something along the lines of</p> <pre><code>{\n    \"id\": \"job-20250829-110739-457\",\n    \"state\": 7,\n}\n{\n    \"id\": \"job-20250829-110620-968\",\n    \"state\": 7,\n}\n...\n</code></pre>"},{"location":"api/api-overview/#geting-information-about-one-job","title":"Geting Information About One Job","text":"<p>You can get more information about a specific job by calling <code>GET /jobs/&lt;job-id&gt;</code>. Following the previous example:</p> <pre><code>curl \\\n    -H \"X-Auth-Token: ${TIS_TOKEN}\" \\\n    'https://imputation.biodatacatalyst.nhlbi.nih.gov/api/v2/jobs/job-20251016-102620-166' \\\n    | jq\n</code></pre> <p>Might produce a response like this:</p> <pre><code>{\n    \"id\": \"job-20251016-102620-166\",\n    \"name\": \"job-20251016-102620-166\",\n    \"state\": 4, // SUCCESS\n    \"positionInQueue\": -1,\n    \"submittedOn\": 1760624780293,\n    \"startTime\": 1760624780394,\n    \"endTime\": 1760626138133,\n    \"deletedOn\": -1,\n    \"steps\": [\n        (log data from each imputation step)\n    ],\n    \"outputParams\": [\n        (information about downloadable output files)\n    ],\n}\n</code></pre>"},{"location":"api/api-overview/#submitting-a-job","title":"Submitting a Job","text":"<p>To submit a job, you call <code>POST /jobs/submit/imputationserver2</code> and set several parameters in the payload, including the whole file(s) you're uploading for imputation. To transfer such large data volumes, you need to use the <code>multipart/form-data</code> encoding. Luckily, <code>curl</code> makes it quite easy:</p> <pre><code>curl \\\n    -H \"X-Auth-Token: ${TIS_TOKEN}\" \\\n    -F \"refpanel=topmed-r3\" \\\n    -F \"build=hg19\" \\\n    -F \"population=all\" \\\n    -F \"files=@chr19.vcf.gz\" \\\n    -F \"files=@chr20.vcf.gz\" \\\n    -F \"files=@chr21.vcf.gz\" \\\n    'https://imputation.biodatacatalyst.nhlbi.nih.gov/api/v2/jobs/submit/imputationserver2'\n</code></pre> <p>The <code>-F</code> flag denotes a form field for <code>multipart/form-data</code>, and the ampersand <code>@</code> tells <code>curl</code> to load the whole file as a binary stream. Once you submit, you will receive a message like this:</p> <pre><code>{\n  \"success\": true,\n  \"message\": \"Your job was successfully added to the job queue.\",\n  \"id\": \"job-20251016-224029-866\"\n}\n</code></pre>"},{"location":"api/api-overview/#notes-on-format","title":"Notes on Format","text":"<ul> <li>Timestamps are milliseconds since the Unix Epoch (January 1st, 1970). If a timestamp has not been set (for example, the deletion time for a still-valid job), it's returned as <code>-1</code>.</li> <li>The job state is an enumeration, see the API reference for details.</li> <li>There are many additional fields we do not document here. They are not considered useful for an end-user.</li> </ul>"},{"location":"api/api-reference/","title":"API Reference","text":""},{"location":"api/api-reference/#authentication","title":"Authentication","text":"<p>All HTTP calls to the API require authentication with a bearer token, sent in the request header <code>X-Auth-Token</code>. See the API Overview for details on obtaining a token.</p> <p>The token you receive from the server is a JSON Web Token (JWT), as specified by the IETF standard RFC 7519. For a good introduction to the format, see here.</p>"},{"location":"api/api-reference/#data-models","title":"Data Models","text":""},{"location":"api/api-reference/#job-info","title":"Job Info","text":"<p>This data type is returned by several API points. Depending on how much data is available, it can be a large, nested object.</p> <pre><code>JobInfo = {\n    id: str, // Unique ID for this job (form: \"job-&lt;timestamp&gt;\")\n    name: str, // User-defined job name (optional; defaults to `id`)\n    state: int, // See \"State format\" below\n    positionInQueue: int, // -1 if not queueing, position in queue otherwise\n    submittedOn: int, // Submission time (milliseconds since the Unix Epoch)\n    startTime: int, // -1 if not started, otherwise start time (milliseconds since the Unix Epoch)\n    endTime: int, // -1 if not done, otherwise end time (milliseconds since the Unix Epoch)\n    deletedOn: int, // -1 if not deleted, otherwise deletion time (milliseconds since the Unix Epoch)\n    steps: [StepInfo], // See StepInfo details below\n    outputParams: [OutputInfo], // See OutputInfo details below\n    ... // Additional fields\n}\n</code></pre>"},{"location":"api/api-reference/#state-format","title":"State Format","text":"<p>The <code>state</code> field in <code>JobInfo</code> is an enumeration. These are its possible values:</p> <pre><code>DEAD             = -1\nWAITING          =  1\nRUNNING          =  2\nEXPORTING        =  3\nSUCCESS          =  4\nFAILED           =  5\nCANCELED         =  6\nRETIRED          =  7\nSUCCESS_NOTIFIED =  8\nFAIL_NOTIFIED    =  9\nDELETED          = 10\n</code></pre>"},{"location":"api/api-reference/#step-logs","title":"Step Logs","text":"<p>The <code>StepInfo</code> model corresponds to the <code>steps</code> list in <code>JobInfo</code>, and contains log information for each step in the imputation process (input validation, quality control, phasing, etc.)</p> <pre><code>StepInfo = {\n    name: str, // Which step is this?\n    logMessages: [ // List of log messages produced by this step\n        {\n            message: str, // Log contents\n            time: int, // When was this message produced? (milliseconds sine Unix Epoch)\n            ... // Additional fields\n        }\n    ],\n    ... // Additional fields\n}\n</code></pre>"},{"location":"api/api-reference/#output-files","title":"Output Files","text":"<p>The <code>OutputInfo</code> model corresponds to the <code>outputParams</code> list in <code>JobInfo</code>, and contains grouped file information for the available downloads.</p> <pre><code>OutputInfo = {\n    name: str, // \"output\" or \"cloudgene_logs\"\n    description: str, // Descriptive name of the output\n    files: [ // List of specific file data\n        {\n            name: str, // File name\n            size: str, // File size, as \"&lt;number&gt; &lt;units&gt;\"\n            hash: str, // Required to get the download link\n        }\n    ],\n    ... // Additional fields\n}\n</code></pre>"},{"location":"api/api-reference/#endpoints","title":"Endpoints","text":"<p>The following sections give detailed information and examples about the most common endpoints you will interact with.</p> <p>Because the authentication token should be kept secure, in all the following examples we assume you haven't copy-pasted yours in a script. Instead, we assume you have set it as an enviornment variable called <code>TIS_TOKEN</code>.</p>"},{"location":"api/api-reference/#list-your-jobs","title":"List Your Jobs","text":""},{"location":"api/api-reference/#request","title":"Request","text":"<p><code>GET /jobs</code></p> <ul> <li>URL parameters:<ul> <li><code>page: int</code> (optional) \u2014 only retrieve the selected page (15 entries per page).</li> </ul> </li> </ul>"},{"location":"api/api-reference/#response","title":"Response","text":"<pre><code>ListJobsResponse = {\n    count: int, // Total number of jobs\n    page: int, // Page number (1-indexed)\n    pageSize: int, // Number of entries per page (15 if paginating, same as `count` otherwise)\n    data: [JobInfo], // List of jobs (see Data Models)\n    ... // Additional fields\n}\n</code></pre>"},{"location":"api/api-reference/#example","title":"Example","text":"<p>Request:</p> CURLPython <pre><code>$ curl \\\n    -H \"X-Auth-Token: ${TIS_TOKEN}\" \\\n    'https://imputation.biodatacatalyst.nhlbi.nih.gov/api/v2/jobs?page=3'\n</code></pre> <pre><code>import os, json\nimport requests\n\nBASE_URL = \"https://imputationserver.sph.umich.edu/api/v2\"\nAUTH_TOKEN = os.environ(\"TIS_TOKEN\") # Reading from environment (don't store it in code!)\n\n# List the user's jobs\nresponse = requests.get(BASE_URL + \"/jobs?page=3\", headers={'X-Auth-Token' : AUTH_TOKEN })\n\nif response.ok:\n    payload = response.json()\n    print(json.dumps(payload, indent=4))\n</code></pre> <p>Response:</p> <pre><code>{\n    \"count\": 32,\n    \"page\": 3,\n    \"pageSize\": 15,\n    \"data\": [\n        {\n            \"id\": \"job-20250829-110739-457\",\n            \"name\": \"My Job!\",\n            \"state\": 7, // RETIRED\n            \"positionInQueue\": -1,\n            \"submittedOn\": 1756480059559,\n            \"startTime\": 1756480059670,\n            \"endTime\": 1756480937680,\n            \"deletedOn\": 1757087819500,\n            \"steps\": [],\n            \"outputParams\": [],\n        },\n        {\n            \"id\": \"job-20250829-110620-968\",\n            \"name\": \"Another One Bites the Dust\",\n            \"state\": 7, // RETIRED\n            \"positionInQueue\": -1,\n            \"submittedOn\": 1756480059088,\n            \"startTime\": 1756480059200,\n            \"endTime\": 1756480979111,\n            \"deletedOn\": 1757087819674,\n            \"steps\": [],\n            \"outputParams\": [],\n        }\n    ]\n}\n</code></pre>"},{"location":"api/api-reference/#get-details-for-one-job","title":"Get Details for One Job","text":""},{"location":"api/api-reference/#request_1","title":"Request","text":"<p><code>GET /jobs/&lt;job-id&gt;</code></p>"},{"location":"api/api-reference/#response_1","title":"Response","text":"<p><code>JobInfo</code> (see Data Models).</p>"},{"location":"api/api-reference/#example_1","title":"Example","text":"<p>Request:</p> CURLPython <pre><code>$ curl \\\n    -H \"X-Auth-Token: ${TIS_TOKEN}\" \\\n    'https://imputation.biodatacatalyst.nhlbi.nih.gov/api/v2/jobs/job-20251016-102620-166'\n</code></pre> <pre><code>import os, json\nimport requests\n\nBASE_URL = \"https://imputationserver.sph.umich.edu/api/v2\"\nAUTH_TOKEN = os.environ(\"TIS_TOKEN\") # Reading from environment (don't store it in code!)\nJOB_ID = \"job-20251016-102620-166\"\n\n# Get details for job job-20251016-102620-166\nresponse = requests.get(BASE_URL + f\"/jobs/{JOB_ID}\", headers={'X-Auth-Token' : AUTH_TOKEN })\n\nif response.ok:\n    payload = response.json()\n    print(json.dumps(payload, indent=4))\n</code></pre> <p>Response:</p> <pre><code>{\n    \"id\": \"job-20251016-102620-166\",\n    \"name\": \"job-20251016-102620-166\",\n    \"state\": 4, // SUCCESS\n    \"positionInQueue\": -1,\n    \"submittedOn\": 1760624780293,\n    \"startTime\": 1760624780394,\n    \"endTime\": 1760626138133,\n    \"deletedOn\": -1,\n    \"steps\": [\n        {\n            \"name\": \"Input Validation\",\n            \"logMessages\": [\n                {\n                    \"message\": \"1 valid VCF file(s) found.\\nSamples: 51\\nChromosomes: 20\\nSNPs: 7824\\nChunks: 4\\nDatatype: unphased\\nBuild: hg19\\nReference Panel: topmed-r3-prod (hg38)\\nPopulation: all\\nPhasing: eagle\\nMode: imputation\",\n                    \"time\": 1760626138198,\n                }\n            ],\n        },\n        {\n            \"name\": \"Quality Control\",\n            \"logMessages\": [\n                {\n                    \"message\": \"Uploaded data is hg19 and reference is hg38.\",\n                    \"time\": 1760626138212,\n                },\n                {\n                    \"message\": \"Lift Over\",\n                    \"time\": 1760626138216,\n                },\n                ...\n            ],\n        },\n        ...\n    ],\n    \"outputParams\": [\n        {\n            \"name\": \"output\",\n            \"description\": \"Downloads\",\n            \"files\": [\n                {\n                    \"name\": \"chr_20.zip\",\n                    \"hash\": \"4acf1c84b465584f2c0c7cc4be74b5704eb1e93c3b5c088ef08b46f833b3ed14\",\n                    \"size\": \"314 MB\",\n                },\n                {\n                    \"name\": \"qc_report.txt\",\n                    \"hash\": \"7207fe05a74a0686e568b5cb5d702bd28ee18cb40008a9a22658d78a2b3eeb19\",\n                    \"size\": \"760 bytes\",\n                },\n                ...\n            ],\n        },\n        {\n            \"name\": \"cloudgene_logs\",\n            \"description\": \"Logs\",\n            \"files\": [\n                {\n                    \"name\": \"step1-nextflow.log\",\n                    \"hash\": \"d9c28a3346d31f273dbb23305208bd8009cfa57d180bfac9d83139ed7ec69767\",\n                    \"size\": \"54 KB\",\n                },\n                ...\n            ],\n        }\n    ],\n}\n</code></pre>"},{"location":"api/api-reference/#submit-a-job","title":"Submit a Job","text":""},{"location":"api/api-reference/#request_2","title":"Request","text":"<p><code>POST /jobs/submit/imputationserver2</code></p> <p>This <code>POST</code> request needs to follow the <code>multipart/form-data</code> encoding to pass its inputs. It expects the following form parameters:</p> Name Type Required Default Description files octet stream (input file) mandatory - VCF input file uploaded to the server. This argument can be repeated any number of times, once for each input file. refpanel <code>topmed-r3</code> mandatory - Reference panel (only TOPMed r3 available). population <code>off</code> or <code>all</code> mandatory - Allele frequency check. build <code>hg19</code> or <code>hg38</code> optional <code>hg19</code> Build format of the input VCF files. job-name string optional (job ID) User-defined name for this job. mode <code>imputation</code> or <code>qc_only</code> optional <code>imputation</code> Whether to only run QC or full imputation. phasing <code>eagle</code> or <code>no_phasing</code> optional <code>eagle</code> Whether to phase with Eagle or skip phasing. r2Filter float optional <code>0</code> Equivalent to the \"rsq Filter\" option in the UI. password string optional (random) If provided, will be used as the password for all the ZIP files generated by this job. By default, a random secure password is generated and emailed to the user's address."},{"location":"api/api-reference/#response_2","title":"Response","text":"<pre><code>SubmitJobResponse = {\n    success: bool, // Was this request accepted for processing?\n    message: str, // On failure: error reason. On success: generic message\n    id: str, // The job ID for the newly submitted job\n}\n</code></pre> <p>Since most input validation is done when the job gets processed, this call usually succeeds even when there are major issues such as missing form fields. Therefore, the only important aspect of the response is the returned job <code>id</code>.</p>"},{"location":"api/api-reference/#example_2","title":"Example","text":"<p>Request:</p> CURLPython <pre><code>$ curl \\\n    -H \"X-Auth-Token: ${TIS_TOKEN}\" \\\n    -F \"refpanel=topmed-r3\" \\\n    -F \"build=hg19\" \\\n    -F \"population=all\" \\\n    -F \"files=@chr19.vcf.gz\" \\\n    -F \"files=@chr20.vcf.gz\" \\\n    -F \"files=@chr21.vcf.gz\" \\\n    'https://imputation.biodatacatalyst.nhlbi.nih.gov/api/v2/jobs/submit/imputationserver2'\n</code></pre> <pre><code>import os, json\nimport requests\n\nBASE_URL = \"https://imputationserver.sph.umich.edu/api/v2\"\nAUTH_TOKEN = os.environ(\"TIS_TOKEN\") # Reading from environment (don't store it in code!)\n\nurl = BASE_URL + \"/jobs\"\n\nparams = [\n    # header        file            value                       mimetype\n    (\"refpanel\"  , (None          , \"topmed-r3\")),\n    (\"build\"     , (None          , \"hg19\"     )),\n    (\"population\", (None          , \"all\"      )),\n    (\"files\"     , (\"chr19.vcf.gz\", open(\"chr19.vcf.gz\", \"rb\"), \"application/octet-stream\")),\n    (\"files\"     , (\"chr20.vcf.gz\", open(\"chr20.vcf.gz\", \"rb\"), \"application/octet-stream\")),\n    (\"files\"     , (\"chr21.vcf.gz\", open(\"chr21.vcf.gz\", \"rb\"), \"application/octet-stream\")),\n]\n\n# Submit a job with VCF files chr19.vcf.gz, chr20.vcf.gz, chr21.vcf.gz\nresponse = requests.post(url, files=params)\n\nif response.ok:\n    payload = response.json()\n    print(json.dumps(payload, indent=4))\n</code></pre> <p>Response:</p> <pre><code>{\n  \"success\": true,\n  \"message\": \"Your job was successfully added to the job queue.\",\n  \"id\": \"job-20251016-224029-866\"\n}\n</code></pre>"},{"location":"api/api-reference/#cancel-a-job","title":"Cancel a Job","text":""},{"location":"api/api-reference/#request_3","title":"Request","text":"<p><code>GET /jobs/&lt;job-id&gt;/cancel</code></p>"},{"location":"api/api-reference/#response_3","title":"Response","text":"<p><code>JobInfo</code> (see Data Models).</p>"},{"location":"api/api-reference/#example_3","title":"Example","text":"<p>Request:</p> CURLPython <pre><code>$ curl \\\n    -H \"X-Auth-Token: ${TIS_TOKEN}\" \\\n    'https://imputation.biodatacatalyst.nhlbi.nih.gov/api/v2/jobs/job-20251016-153844-544/cancel'\n</code></pre> <pre><code>import os, json\nimport requests\n\nBASE_URL = \"https://imputationserver.sph.umich.edu/api/v2\"\nAUTH_TOKEN = os.environ(\"TIS_TOKEN\") # Reading from environment (don't store it in code!)\nJOB_ID = \"job-20251016-102620-166\"\n\n# Get details for job job-20251016-102620-166\nresponse = requests.get(\n    url=BASE_URL + f\"/jobs/{JOB_ID}/cancel\",\n    headers={'X-Auth-Token' : AUTH_TOKEN },\n)\n\nif response.ok:\n    payload = response.json()\n    print(json.dumps(payload, indent=4))\n</code></pre> <p>Response:</p> <pre><code>{\n  \"id\": \"job-20251016-153844-544\",\n  \"name\": \"job-20251016-153844-544\",\n  \"state\": 6, // CANCELED\n  \"positionInQueue\": 0,\n  \"submittedOn\": 1760643524656,\n  \"startTime\": 1760643524760,\n  \"endTime\": 1760644443152,\n  \"deletedOn\": -1,\n  \"steps\": [\n    {\n      \"name\": \"Input Validation\",\n      \"logMessages\": [\n        {\n          \"message\": \"1 valid VCF file(s) found.\\nSamples: 51\\nChromosomes: 20\\nSNPs: 7824\\nChunks: 4\\nDatatype: unphased\\nBuild: hg19\\nReference Panel: topmed-r3 (hg38)\\nPopulation: all\\nPhasing: eagle\\nMode: imputation\",\n          \"success\": true,\n        }\n      ]\n    },\n    ...\n  ],\n  \"outputParams\": [\n    {\n      \"name\": \"output\",\n      \"description\": \"Downloads\",\n      \"files\": [\n        {\n          \"name\": \"statistics/lift-over.txt\",\n          \"hash\": \"2649d002420642a92575954678b91801b7a011900a848989d14611fc5251ff7a\",\n          \"size\": \"0 bytes\",\n        },\n        ...\n      ],\n    }\n  ],\n}\n</code></pre>"},{"location":"api/api-reference/#download-a-file","title":"Download a File","text":""},{"location":"api/api-reference/#request_4","title":"Request","text":"<p><code>GET https://imputation.biodatacatalyst.nhlbi.nih.gov/share/results/&lt;file-hash&gt;/&lt;file-name&gt;</code></p> <p>Not under the same base as the other URLs!</p> <p>You will need to retrieve the file hash and file name from <code>outputParams</code> as returned by <code>GET jobs/&lt;job-id&gt;</code>, see Data Models.</p>"},{"location":"api/api-reference/#response_4","title":"Response","text":"<p>This endpoint returns the file contents. Note that this can be a large binary blob! No JSON is produced.</p>"},{"location":"api/api-reference/#example_4","title":"Example","text":"CURLPython <pre><code>$ curl \\\n    -H \"X-Auth-Token: ${TIS_TOKEN}\" \\\n    -L \\\n    'https://imputation.biodatacatalyst.nhlbi.nih.gov/share/results/&lt;hash&gt;/qc_report.txt' \\\n    &gt; qc_report.txt\n</code></pre> <pre><code>import os, json\nimport requests\n\nBASE_URL = \"https://imputationserver.sph.umich.edu\" # Note the shorter BASE_URL\nAUTH_TOKEN = os.environ(\"TIS_TOKEN\") # Reading from environment (don't store it in code!)\nHASH = \"...\" # Put your file hash here (or better yet, write code that does that for you!)\nFILE = \"qc_report.txt\"\n\nurl = BASE_URL + f\"/share/results/{HASH}/{FILE}\"\nheaders = {'X-Auth-Token' : AUTH_TOKEN }\n\n# Use `stream=True` to iterate one chunk at a time, useful for big files!\nwith requests.get(url, headers=headers, stream=True) as response:\n    with open(FILE, \"wb\") as file:\n        for chunk in response.iter_content(chunk_size=8192):\n            file.write(chunk)\n</code></pre> <p>No response this time: the above call results in a local file <code>./qc_report.txt</code> containing the Quality Control report.</p>"},{"location":"imputation/data-preparation/","title":"Data Preparation","text":"<p>The TOPMed Imputation Server accepts VCF files compressed with bgzip. Please ensure that the following requirements are met:</p> <ul> <li>Create a separate vcf.gz file for each chromosome.</li> <li>Variants must be sorted by genomic position.</li> <li>GRCh37 or GRCh38 coordinates are required.</li> <li>If your input data is GRCh37/hg19, please ensure chromosomes are encoded without prefix (e.g. 20).</li> <li>If your input data is GRCh38/hg38, please ensure chromosomes are encoded with prefix 'chr' (e.g. chr20).</li> <li>VCF files need to be version 4.2 (or lower). This is specified in the VCF file header section.</li> <li>Must contain GT field in the FORMAT column. All other FORMAT fields will be ignored. (if you are seeing problems with very large uploads, it may help to remove other FORMAT fields)</li> <li>Due to server resource requirements, there is a maximum of 25,000 samples per chromosome per job (and a minimum of 20 samples). Please see the FAQ for details.</li> </ul> <p>Note</p> <p>Multiple *.vcf.gz files can be uploaded at once.</p>"},{"location":"imputation/data-preparation/#quality-control-for-hrc-1000g-and-caapa-imputation","title":"Quality Control for HRC, 1000G and CAAPA Imputation","text":"<p>Will Rayner provides an excellent toolbox for preparing data: HRC or 1000G Pre-imputation Checks.</p> <p>The main steps for using HRC are:</p>"},{"location":"imputation/data-preparation/#download-tool-and-sites","title":"Download Tool and Sites","text":"<pre><code>wget http://www.well.ox.ac.uk/~wrayner/tools/HRC-1000G-check-bim-v4.2.7.zip\nwget ftp://ngs.sanger.ac.uk/production/hrc/HRC.r1-1/HRC.r1-1.GRCh37.wgs.mac5.sites.tab.gz\n</code></pre>"},{"location":"imputation/data-preparation/#convert-pedmap-to-bed","title":"Convert ped/map to bed","text":"<pre><code>plink --file &lt;input-file&gt; --make-bed --out &lt;output-file&gt;\n</code></pre>"},{"location":"imputation/data-preparation/#create-a-frequency-file","title":"Create a Frequency File","text":"<pre><code>plink --freq --bfile &lt;input&gt; --out &lt;freq-file&gt;\n</code></pre>"},{"location":"imputation/data-preparation/#execute-script","title":"Execute Script","text":"<pre><code>perl HRC-1000G-check-bim.pl -b &lt;bim file&gt; -f &lt;freq-file&gt; -r HRC.r1-1.GRCh37.wgs.mac5.sites.tab -h\nsh Run-plink.sh\n</code></pre>"},{"location":"imputation/data-preparation/#create-vcf-using-vcfcooker","title":"Create VCF Using VcfCooker","text":"<pre><code>vcfCooker --in-bfile &lt;bim file&gt; --ref &lt;reference.fasta&gt;  --out &lt;output-vcf&gt; --write-vcf\nbgzip &lt;output-vcf&gt;\n</code></pre>"},{"location":"imputation/data-preparation/#additional-tools","title":"Additional Tools","text":""},{"location":"imputation/data-preparation/#convert-pedmap-files-to-vcf-files","title":"Convert ped/map Files to VCF Files","text":"<p>Several tools are available:  plink2,  BCFtools or VcfCooker.</p> <pre><code>plink --ped study_chr1.ped --map study_chr1.map --recode vcf --out study_chr1\n</code></pre> <p>Create a sorted vcf.gz file using bcftools:</p> <pre><code>bcftools sort study_chr1.vcf -Oz -o study_chr1.vcf.gz\n</code></pre>"},{"location":"imputation/data-preparation/#checkvcf","title":"CheckVCF","text":"<p>Use checkVCF to ensure that the VCF files are valid. CheckVCF provides \u201cAction Items\u201d (e.g., uploading to an SFTP server) that can be ignored. Focus solely on verifying the validity of the files with this tool.</p> <pre><code>checkVCF.py -r human_g1k_v37.fasta -o out mystudy_chr1.vcf.gz\n</code></pre>"},{"location":"imputation/data-security/","title":"Data Security","text":"<p>The TOPMed Imputation Server stores all data in a secure server hosted on Amazon Web Services (AWS). As of May 2023, we have completed a rigorous security review and received a federal Authorization to Operate (ATO) from NIH/NHLBI. A wide array of security measures are in force:</p> <ul> <li>All interactions with the server are secured with HTTPS.</li> <li>Input data is deleted from our servers as soon as it is no longer needed.</li> <li>We only store the number of samples and markers analyzed; we do not access your data in any way.</li> <li>All results are encrypted with a strong one-time password, ensuring that only you can access them. We do not store the password.</li> <li>After imputation is complete, the user has 7 days to retrieve the results using an encrypted connection. The data is automatically deleted at the end of this period.</li> <li>The complete source code is available via public GitHub repositories:<ul> <li>statgen/cloudgene3</li> <li>statgen/imputationserver2</li> </ul> </li> </ul>"},{"location":"imputation/data-security/#who-has-access","title":"Who Has Access?","text":"<p>To upload and download genotype data, users must register with a unique email address and a strong password. Each user can only download imputation results for samples they have uploaded themselves; other users of the Imputation Server will not have access to your data.</p>"},{"location":"imputation/data-security/#what-security-or-firewalls-protect-access","title":"What Security or Firewalls Protect Access?","text":"<p>A wide array of security measures are in force on the imputation servers:</p> <ul> <li>All stored data is encrypted at rest using FIPS 140-2 validated cryptographic software as well as encrypted in transit.</li> <li>Access controls follow the principle of least privilege. All administrative access is secured via two-factor authentication using role-based access controls and temporary credentials.</li> <li>Network access is restricted and filtered via web application firewalls, network access control lists, and security groups. Public/private network segmentation also ensures only the services that need to be are exposed to the public internet. All internal traffic and requests are logged and scanned for malicious or unusual activity.</li> <li>Advanced DDoS protection is in place to assure consistent site availability.</li> <li>All administrative user activities, system activities, and network traffic is logged and scanned for anomalies and malicious activity. Findings are alerted to administrative users.</li> </ul>"},{"location":"imputation/data-security/#what-encryption-of-the-data-is-used-while-the-data-are-present","title":"What Encryption of the Data Is Used While the Data Are Present?","text":"<p>Imputation results are encrypted with a one-time password generated by the system. The password includes lowercase and uppercase letters, special characters, and numbers, with a maximum of three duplicate characters.</p>"},{"location":"imputation/getting-started/","title":"Getting Started","text":"<p>To use the TOPMed Imputation Server, registration is required. We will send an activation email to the provided address. Please follow the instructions in the email to activate your account. If the email does not arrive, ensure you have entered the correct address and check your spam folder.</p> <p>After the email address has been verified, the service can be used without any costs.</p> <p>Please cite the following papers if you use the TOPMed Imputation Server:</p> <ul> <li>TOPMed Study: <p>Taliun, D. et al. (2019) Sequencing of 53,831 diverse genomes from the NHLBI TOPMed Program. Biorxiv, doi:10.1101/563866</p> </li> <li>Imputation Server: <p>Das S, Forer L, Sch\u00f6nherr S, Sidore C, Locke AE, Kwong A, Vrieze S, Chew EY, Levy S, McGue M, Schlessinger D, Stambolian D, Loh PR, Iacono WG, Swaroop A, Scott LJ, Cucca F, Kronenberg F, Boehnke M, Abecasis GR, Fuchsberger C. Next-generation genotype imputation service and methods. Nature Genetics 48, 1284\u20131287 (2016).</p> </li> <li>Minimac Imputation: <p>Fuchsberger, C., Abecasis, G. R., &amp; Hinds, D. A. (2014). minimac2: faster genotype imputation. Bioinformatics, 31(5), 782\u2013784.</p> </li> </ul>"},{"location":"imputation/getting-started/#set-up-your-first-imputation-job","title":"Set Up Your First Imputation Job","text":"<p>Please log in with your credentials and click on the Run tab to start a new imputation job. The submission dialog allows you to specify the properties of your imputation job.</p> <p></p> <p>The following options are available:</p>"},{"location":"imputation/getting-started/#reference-panels","title":"Reference Panels","text":""},{"location":"imputation/getting-started/#topmed-imputation-server","title":"TOPMed Imputation Server","text":"<p>The TOPMed Imputation Server offers genotype imputation for the TOPMed reference panel, which is the largest and most accurate panel available amongst the two imputation servers.</p> <ul> <li>TOPMed (Version r3 2023)</li> </ul> <p>See the reference panel documentation for details.</p>"},{"location":"imputation/getting-started/#michigan-imputation-server","title":"Michigan Imputation Server","text":"<p>The Michigan Imputation Server is a separate service with several additional reference panels available.  Consult the relevant documentation for details.</p>"},{"location":"imputation/getting-started/#upload-vcf-files-from-your-computer","title":"Upload VCF Files From Your Computer","text":"<p>The Input Files dialog lets you choose VCF files for imputation. When you submit the job, all the selected files will be transferred from your local file system to the TOPMed Imputation Server. By clicking on Select Files, an open dialog will appear where you can choose your VCF files.</p> <p></p> <p>You can select multiple files by holding down the Ctrl, Cmd, or Shift key, depending on your operating system. After confirming your selection, all chosen files will be listed in the Input Files dialog.</p> <p></p> <p>Please ensure that all files meet the requirements.</p> <p>Important</p> <p>Since version 1.7.2 URL-based uploads (<code>sftp</code> and <code>http(s)</code>) are no longer supported. Please use direct file uploads instead.</p>"},{"location":"imputation/getting-started/#build","title":"Build","text":"<p>Please select the build of your data. Currently, the supported options are hg19 and hg38. The TOPMed reference panel is based on hg38 coordinates; if you submit hg19, the server will automatically update the genome positions of your data (liftOver).</p>"},{"location":"imputation/getting-started/#rsq-filter","title":"rsq Filter","text":"<p>To minimize the file size, the TOPMed Imputation Server includes an r<sup>2</sup> filter option that excludes all imputed SNPs with an r<sup>2</sup>-value (imputation quality) smaller than the specified threshold.</p>"},{"location":"imputation/getting-started/#phasing","title":"Phasing","text":"<p>If your uploaded data is unphased, Eagle v2.4.1 will be used for phasing. If your VCF file already contains phased genotypes, please select the \u201cNo phasing\u201d option.</p> <p>Important</p> <p>Michigan Imputation Server 2 will rephase your data if a phasing engine is selected.</p> Algorithm Description Eagle v2.4.1 The Eagle algorithm estimates haplotype phase using the HRC reference panel. After phasing or imputation you will receive phased genotypes in your VCF files."},{"location":"imputation/getting-started/#allele-frequency-check","title":"Allele Frequency Check","text":"<p>Please select whether to compare allele frequencies between your data and the reference panel.</p> <p>In case your samples are mixed from different populations, please select Skip to skip the allele frequency check. For mixed populations, no QC-Report will be created.</p> <p>Note</p> <p>In previous versions, this paramter was called \"Population\".</p>"},{"location":"imputation/getting-started/#mode","title":"Mode","text":"<p>Please select if you want to run Quality Control &amp; Imputation or Quality Control Only.</p> <p>Important</p> <p>Results (e.g. QC Report) will be available only after the entire job is complete.</p>"},{"location":"imputation/getting-started/#aes-256-encryption","title":"AES 256 Encryption","text":"<p>All imputed genes are returned as encrypted ZIP files. If you select the AES 256 Encryption option, we will use the stronger AES 256 encryption protocol instead of the default method. Note that AES encryption is not compatible with standard unzip programs. If this option is selected, we recommend using 7-zip to access your results.</p>"},{"location":"imputation/getting-started/#start-your-imputation-job","title":"Start Your Imputation Job","text":"<p>After confirming our Terms of Service, you can submit your imputation job by clicking Start Imputation. If your data passes the QC steps, your job will be added to the queue and processed as soon as possible. You can check your position in the queue on the job summary page.</p> <p></p> <p>We will notify you by email as soon as the job is finished or if your data does not pass the quality control steps.</p>"},{"location":"imputation/getting-started/#input-validation","title":"Input Validation","text":"<p>In the first step, we check if your uploaded files are valid and calculate basic statistics, such as the number of samples, chromosomes, and SNPs.</p> <p></p> <p>After the input validation is complete, basic statistics can be viewed directly in the web interface.</p> <p></p> <p>If you encounter problems with your data, please read this tutorial on Data Preparation to ensure your data is in the correct format.</p>"},{"location":"imputation/getting-started/#quality-control","title":"Quality Control","text":"<p>In this step, we check each variant and exclude it if it meets any of the following criteria:</p> <ol> <li>contains invalid alleles</li> <li>duplicates</li> <li>indels</li> <li>monomorphic sites</li> <li>allele mismatch between reference panel and uploaded data</li> <li>SNP call rate &lt; 90%</li> <li>Strand flips and allele swaps</li> </ol> <p>All filtered variants are listed in a file called <code>statistics.txt</code>, which can be downloaded by clicking the provided link.</p> <p>Important</p> <p>Due to changes in Minimac 4, Michigan Imputation Server 2 now includes filtering and checks for allele swaps. For more information, please click here.</p> <p></p> <p>If you selected a population, we compare the allele frequencies of the uploaded data with those from the reference panel. The results of this check are available in the QC report and can be downloaded by clicking on <code>qcreport.html</code>.</p>"},{"location":"imputation/getting-started/#pre-phasing-and-imputation","title":"Pre-Phasing and Imputation","text":"<p>Imputation is achieved with Minimac4 (version 4.1.6).</p>"},{"location":"imputation/getting-started/#data-compression-and-encryption","title":"Data Compression and Encryption","text":"<p>If imputation is successful, we will compress and encrypt your data, and send you a random password via email.</p> <p></p> <p>Password not stored at any time</p> <p>This password is not stored on our server at any time. Therefore, if you lost the password, there is no way to resend it to you.</p>"},{"location":"imputation/getting-started/#download-results","title":"Download Results","text":"<p>You will be notified by email as soon as the imputation job is complete. For each submitted gene, a ZIP archive containing the imputed results can be downloaded directly from the server. To decrypt the results, a one-time password will be generated by the server and included in the email. The QC report and filter statistics are also available for viewing and download.</p> <p></p> <p>All data is deleted automatically after 7 days</p> <p>Be sure to download all required data within 7 calendar days! We will send you a reminder 48 hours before your data is deleted. Once your job reaches the retired state, we will not be able to recover your data.</p>"},{"location":"imputation/getting-started/#download-via-a-web-browser","title":"Download Via a Web Browser","text":"<p>All results can be downloaded directly through your browser by clicking on the filename.</p> <p></p> <p>If you wish to download results via the command line using <code>wget</code> or <code>curl</code>, click on the download icon to obtain download links. A new dialog will appear. Use the wget tab to get a copy &amp; paste ready command that can be used on Linux or MacOS:</p> <p></p> <p>Otherwise, you can use the URLs tab to copy the download links.</p>"},{"location":"imputation/pipeline-overview/","title":"Pipeline Overview","text":"<p>Our pipeline performs the following steps:</p>"},{"location":"imputation/pipeline-overview/#quality-control","title":"Quality Control","text":"<ul> <li>Create chunks of 10 Mb each.</li> <li> <p>For each 10Mb chunk, perform the following checks:</p> <p>On Chunk Level:</p> <ul> <li>Determine the number of valid variants: A variant is considered valid if it is included in the reference panel. At least 3 variants must be included.</li> <li>Determine the amount of variants found in the reference panel: At least 50% of the variants must be be included in the reference panel.</li> <li>Determine the sample call rate: At least 50% of the variants must be called for each sample.</li> </ul> <p>Chunk exclusion: <pre><code>if (#variants &lt; 3 || overlap &lt; 50% || sampleCallRate &lt; 50%) then (exclude chunk)\n</code></pre></p> <p>On Variant Level:</p> <ul> <li>Check alleles: Only <code>A</code>, <code>C</code>, <code>G</code>, <code>T</code> are allowed.</li> <li>Calculate alternative allele frequency (AF): Mark all variants with an <code>AF &gt; 0.5</code>.</li> <li>Calculate the SNP call rate.</li> <li>Calculate the chi-square for each variant (reference panel vs. study data).</li> <li>Determine allele switches: Compare the reference and alternate alleles of the reference panel with the study data.</li> <li>Determine strand flips: After eliminating possible allele switches, flip and compare the reference and alternate alleles from the reference panel with the study data.</li> <li>Determine allele switches in combination with strand flips: Apply the two rules above together.</li> </ul> <p>Variant exclusion: Variants are excluded in case of: [a] invalid alleles occur (<code>!(A,C,G,T)</code>), [b] duplicates (DUP filter or <code>pos - 1 == pos</code>), [c] indels, [d] monomorphic sites, [e] allele mismatch between reference panel and study, [f] SNP call rate &lt; 90%, [g] more than 100 strand or allele switches.</p> <p>On Sample Level:</p> <ul> <li>A chunk is excluded if any sample within that chunk has a call rate &lt; 50%. Only complete chunks are excluded, not individual samples (see \u201cOn Chunk Level\u201d above).</li> </ul> </li> <li> <p>Perform a liftOver step if the build of the input data does not match the build of the reference panel (e.g., hg37 vs. hg38).</p> </li> </ul>"},{"location":"imputation/pipeline-overview/#phasing","title":"Phasing","text":"<ul> <li>For each chunk, execute one of the following phasing algorithms (using an overlap of 5 Mb). For example, for <code>chr20:1-20,000,000</code> and reference population <code>EUR</code>:</li> </ul> <p>Eagle2</p> <pre><code>./eagle \\\n   --vcfRef HRC.r1-1.GRCh37.chr20.shapeit3.mac5.aa.genotypes.bcf \\\n   --vcfTarget chunk_20_0000000001_0020000000.vcf.gz \\\n   --geneticMapFile genetic_map_chr20_combined_b37.txt \\\n   --outPrefix chunk_20_0000000001_0020000000.phased \\\n   --chrom 20 \\\n   --bpStart 1 \\\n   --bpEnd 25000000 \\\n   --allowRefAltSwap \\\n   --vcfOutFormat z \\\n   --keepMissingPloidyX \\\n   --numThreads ${task.cpus}\n</code></pre> <p>Target-only sites for unphased data are excluded from the final output.</p>"},{"location":"imputation/pipeline-overview/#imputation","title":"Imputation","text":"<ul> <li>For each chunk, execute Minimac4 to impute the phased data (using a window of 500 kb):</li> </ul> <pre><code>    ./Minimac4 \\\n        --region 20:1-20000000 \\\n        --overlap 500000 \\\n        --output ${chunkfile_name}.dose.vcf.gz \\\n        --output-format vcf.gz \\\n        --format GT,DS,GP \\\n        --min-ratio $minimac_min_ratio \\\n        --all-typed-sites \\\n        --meta \\\n        --min-recom 0.00001 \\\n        --min-r2 0.8 \\\n        --sites out.info.gz \\\n        --empirical-output out.empiricalDose.vcf.gz \\\n        --threads 1 \\\n        --decay 0 \\\n        HRC.r1-1.GRCh38.chr1.shapeit3.mac5.aa.genotypes.msav \\\n        chunk_1_0000000001_0020000000.phased.vcf\n</code></pre>"},{"location":"imputation/pipeline-overview/#compression-and-encryption","title":"Compression and Encryption","text":"<ul> <li>Merge all chunks of a chromosome into one single VCF file (<code>.vcf.gz</code>).</li> <li>Encrypt the data with a one-time password.</li> </ul>"},{"location":"imputation/pipeline-overview/#chromosome-x-pipeline","title":"Chromosome X Pipeline","text":"<p>In addition to the standard QC, the following per-sample checks are performed for chrX:</p> <ul> <li>Ploidy Check: Verifies if all variants in the non-PAR region are either haploid or diploid.</li> <li>Mixed Genotypes Check: Verifies if the proportion of mixed genotypes (e.g., 1/.) is &lt; 10%.</li> </ul> <p>For phasing and imputation, chrX is divided into three independent chunks (PAR1, non-PAR, PAR2). These chunks are then automatically merged by the TOPMed Imputation Server and returned as a single complete chromosome X file.</p>"},{"location":"imputation/qc-allele-swaps/","title":"Changes in Allele Swaps Handling in Imputation Server 2.0","text":"<p>With the update to the latest Minimac4 version, we have modified the default handling of allele swaps in Michigan Imputation Server 2. </p>"},{"location":"imputation/qc-allele-swaps/#overview-of-changes","title":"Overview of Changes","text":"<p>In previous versions of the server, swapped reference and alternate alleles were automatically corrected, with only ambiguous SNPs being filtered out. To improve genotype quality and avoid errors from incorrectly genotyped variants, the handling of allele swaps has now been updated.</p>"},{"location":"imputation/qc-allele-swaps/#key-changes","title":"Key Changes","text":"<ul> <li>Previous Behavior: Allele swaps were automatically corrected, with only ambiguous SNPs being filtered out.</li> <li>New Behavior: With the update, the server now enforces stricter quality control. The system allows up to 100 allele swaps during processing. If more than 100 allele swaps are detected, the quality control process will fail, stopping the imputation process to prevent poor genotype quality.</li> </ul>"},{"location":"imputation/qc-allele-swaps/#impact-on-data-submissions","title":"Impact on Data Submissions","text":"<p>This update may cause failures in submissions that were previously accepted but now exceed the 100-allele swap threshold.</p>"},{"location":"imputation/qc-allele-swaps/#recommendations","title":"Recommendations","text":"<p>To avoid imputation quality issues and ensure your data meets the new allele swaps threshold:</p> <ul> <li> <p>We recommend using tools such as Will Rayner\u2019s tool to check for and correct allele switches in your dataset.</p> </li> <li> <p>This will ensure that issues such as strand or allele flips are resolved before submission, helping your data meet the new quality standards.</p> </li> </ul> <p>Please follow these steps to verify your data before submitting it to Imputation Server 2.</p>"},{"location":"imputation/reference-panel/","title":"Reference Panel","text":""},{"location":"imputation/reference-panel/#reference-panel","title":"Reference Panel","text":"<p>We currently offer one reference panel for imputation: TOPMed r3.</p>"},{"location":"imputation/reference-panel/#topmed-r3","title":"TOPMed r3","text":"Number of Samples 133,597 Sites (chr1-22) 445,600,184 Chromosomes 1-22, X Imputation Server https://imputation.biodatacatalyst.nhlbi.nih.gov/ Website https://topmed.nhlbi.nih.gov/"},{"location":"imputation/reference-panel/#other-reference-panels","title":"Other Reference Panels","text":"<p>Our sister service at the Michigan Imputation Server offers additional reference panels, see here for more details.</p>"}]}